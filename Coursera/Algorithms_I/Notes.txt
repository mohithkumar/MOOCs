Week 1:

Upshot of an algorithm is the running time which is the number of operations.

Worst-case analysis : No assumption is made about the problem/input. 
Average-case analysis : Require domain knowledge of the problem/input.
Benchmarks : Require domain knowledge of the problem; Eg an assumption is made (or) an understanding is required about the size of inputs etc.
Asymptotic analysis : refers to the performance or running time of an algorithm as N (number of inputs) tends to infinity.

A Fast algorithm is one whose worst-case running time grows slowly with input size.

It is preferred to do Worst-case analysis using Asymptotic running time.

Mergesort - 
Time complexity == 6NlogN ---> where 6N is the work per level and logN is the number of levels.

Running Time:
For every input	array of N numbers, MergeSort produces a sorted output array and uses atmost 6NlogN + 6N operations.

At each level J = 0, 1, 2, .., N, there are 2^J subproblems, each of size N/(2^J).

Merge Sort Pseudo Code - 
C = Output array [length = n]
A = 1st sorted array [n/2]
B = 2nd sorted array [n/2]
i = 1; j = 1

for k = 1 to n
  if A[i] < B[j]
    C[k] = A[i]
    i++
  elif B[j] < A[i]
    C[k] = B[j]
    j++
end

===============================================================================

Week 2:
Time Complexity of Quicksort on an already sorted array is O(N) 
{where N is the number of elements in the array and the pivot element is the first element of the array}

On a random array input, time complexity of Quicksort is O(N LogN) where pivot is the median element of the subarray during every recursive call.

Ideally, it is best to choose the pivot randomly in every recursive call. The time complexity would therefore be O(N LogN)

Quick Sort Pseudo Code - 

===============================================================================

Week 3:
Karger Min Cut Pseudo Code -
    While there are more than 2 vertices:
    •  pick a remaining edge (u,v) uniformly at random
    •  merge (or “contract” ) u and v into a single vertex
    •  remove self-loops
    return cut represented by final 2 vertices.

===============================================================================

Week 4:
Undirected Graphs - can be traversed in both directions.
Directed Graphs - can be traversed in one direction only.

Idea behind Graph search is to do it as effiiently as possible i.e, don't explore anything twice.
Generic algorithm for Graph Search: 
- Given Graph G (Directed or Undirected) and start vertex s.
- Initially s explored, all other vertices unexplored.
- While possible:
    - Choose an edge (u, v) with u explored and v unexplored.
        - If none, halt.
    - Mark v as explored.
At end of the algorithm, v vertices are explored; in other words, G has a path P from s to v.

BFS uses Queue while DFS uses Stack.

Breadth First Search [M edges & N Vertices] - 
- Explore nodes in "layers"
- Can compute shortest paths
- Can compute connected components of an undirected graph
- Time complexity O(M+N) using Queue (FIFO) as underlying data structure
Given Graph G and start vertex s.
- Initially s explored, all other vertices unexplored.
- Let Q [Queue data structure] be initialized with s.
- While Q != 0:
    - remove the first node of Q, call it v
    - for each edge (v, w):
        - if w unexplored:
            - mark w as explored
            - add w to Q (at the end)
At end of the algorithm, v vertices are explored; in other words, G has a path P from s to v.
Running time of main while loop == O(Ns + Ms) where Ns - number of nodes reachable from s
                                                    Ms - number of nodes reachable from s

Shortest Path (for BFS only):
Goal - Compute dist(v): the fewest number of edges on a path from s to v.
Extra Code - Initialize dist(v) = {0 if v = s
                                  {Inf if v != s 
- When considering edge(v, w):
    - if w unexplored, the set dist(w) = dist(v) + 1
Claim - At termination, dist(v) = i <==> v in ith layer [i.e., shortest s-v path has i edges]
Proof Idea - Every layer-i node w is added to Q by a layer-(i-1) node v via the edge (v, w)

Undirected Connectivity
- All nodes unexplored [assume nodes are labelled 1 to N]
- for i:= 1 to N
    - if i not yet explored
        - BFS(G, i)

Depth First Search [M edges & N Vertices] - 
- Explore aggressively (like a maze); only backtrack when necessary
    - Explore aggressively == Go as deeply as you can
- Compute topological ordering of a directed acyclic graph 
- Compute connected components in directed graphs
- Time complexity O(M+N) using Stack (LIFO) as underlying data structure

Recursive Version of DFS (Graph G, start vertex s):
- Mark s as explored
- For every edge (s, v):
    - if v unexplored
        - DFS (G, v)
Claim - At end of the algorithm, v vertices are explored; in other words, G has a path P from s to v.
Running time of main while loop == O(Ns + Ms) where Ns - number of nodes reachable from s
                                                    Ms - number of nodes reachable from s

Topological ordering of a directed graph: is an ordering of the vertices of a graph so that all of the arcs, the directed edges of the graph, only go forward in the ordering.
A topological ordering of a directed graph G is a labelling f of G's nodes such that:
a) the f(v)'s are the set {1, 2, ..., N}
b) (u, v) in G ==> f(u) < f(v)
Time complexity is O(M + N)

If G has directed cycle ===> no topological ordering.

Every directed acycle graph has a sink vertex i.e., a vertex with no outgoing arrows/arcs.
All arcs should go forward; if any arcs go backwards, it is a violation.

To compute Topological ordering:
- Let v be a sink vertex of G
- Set f(v) = N
- Recurse on G - [v]

DFS (Graph G, Start vertex s)
- Mark s explored
- For every edge (s, v):
    - if v not yet explored:
        - DFS (G, v)
- Set f(s) = current_label
- current_label -= 1

DFS-Loop (Graph G)
- Mark all nodes unexplored
- current_label = n // [to keep track of ordering]
- for each vertex v in G:
    - if v not yet explored:
        - DFS (G, v)


===============================================================================

Week 5:

Dijkstra's Shortest Path algorithm - works in any directed graph with non-negative edge lengths.
It computes the shortest paths from a source vertex to all other vertices.

Input: 
- Directed graph G=(V, E). (m = |E|, n = |V|)
- Each edge has non-negative length
- Source vertex s; destination vertex v.
Output:
- for each v in V, compute L(v):= length of a shortest s-v path in G.
- Length of path == Sum of edge lengths
Assumptions:
- for every v in V, there is a directed path from s to v.

BFS can also compute shortest paths (in linear time) only if the length of every edge is 1.

Algorithm:
- Initialize X = [s] where X is the list of vertices processed so far
    in other words, X is the list of vertices for which the shortest path has been computed
- A[s] = 0, where A is the array of computed shortest path distances [sum of the length of edges]
                  A will have one entry for each possible destination vertex v
- B[s] = empty path, B is the array of computed shortest paths [sum of the number of edges]
                     B will have one entry for each possible destination vertex v
** B will not be there in the actual algorithm; it is there for explanation only

Main loop:
While X != V:    // need to grow X by one node
    Among all edges (v, w) in E with v in X and w !in X, pick the edge that minimizes A[v] + length(v, w)
        A[v] + length(v, w) ---> is called Dijkstra's Greedy Criterion
        Call it (v*, w*)
    Add w* to X
    Set A[w*] := A[v*] + length(v*, w*)
    Set B[w*] := B[v*] v(v*, w*)


Time complexity of Dijkstra's algorithm using straight forward implementation is O(mn).
Time complexity of Dijkstra's algorithm using Heaps is O(m log n).

-------------------------------------------------------------------------------

Heap: performs insert, extract minimum operations in O(log n)
- Container for objects that have keys
- Conceptually, a perfectly balanced binary tree [Height of such a tree == log N]
- Heap property: at every node, key <= children's keys
- Extract minimum by swapping up last leaf, bubbling down
- Insert via bubbling up
Also: Will need ability to delete from middle of heap. (bubble up or down as needed)

Invariant #1: Elements in Heap = Vertices of (V - X).

Invariant #2: For every v not in X, key[v] = smallest Dijkstra greedy score of an edge (w, v) in E with w in X.

By invariants, extract-min yields correct vertex w* to add to X next.
               and we set A[w*] to key[w*]

When w extracted from heap [i.e., added to X]
- for each edge (w, v) in E:
    - if v in (V - X) [i.e., in heap]
        - delete v from heap
        - recompute key[v] = min(Key[v], A[w] + length(w, v))

There would be (n - 1) extract mins
Each edge (v, w) triggers at most one Delete/Insert combo (if v added to X first)
So, the numberof heap operations is O(m)
So, running time is O(m log n)

Time taken to Heapify a bunch of objects: O(n)
Time taken to delete a particular object: O(log n)

Heap is the fast way to do repeated minimum computations.

Heap Sort == O(n log n)
- Heapify the given array of elements
- Extract minimum one by one

Priority Queue == Heap

Data Structures: point is to organize data so that it can be accessed quickly and usefully.

Balanced Binary Search Tree:
- Search  == O(log n)
- Select  == O(log n)
- Min/Max == O(log n)
- Insert  == O(log n)
- Delete  == O(log n)

BST:
- One node per key
- Each node has 3 pointers: left child, right child & parent
- left subtree < node < right subtree
- Height [or Depth] is the longest number hops from root to leaf
- Height could vary between log N for a perfectly balanced tree and N for a worst case


